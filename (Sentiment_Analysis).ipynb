{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6NTPS6Y8vI4"
      },
      "source": [
        "# Importing the tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RwqCfJKR8vI5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sklearn\n",
        "import sklearn.linear_model\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn import grid_search\n",
        "from sklearn import tree\n",
        "from sklearn import model_selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92-zKuW98vI8"
      },
      "source": [
        "# Reading in Data\n",
        "Using the function readFile(filename), we will read the contents of a text file and output the contents of the file as a list containing single words. Using this function, we will read all the files into a Pandas Dataframe, in which each row represents a text file and the columns contain the counts of each word in that specific text file. Note that there should be a column for every possible word that occurs throughout all text files, so all the columns together form the unique vocabulary for the dataset. If a word does not appear in a particular file, let its count be 0. In addition, add a column in this Dataframe with the document label, containing either the value ’positive’ or ’negative’. You may also want to add a column with the file name, so you can later check which reviews were incorrectly classified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6u_xFHQb8vI-"
      },
      "outputs": [],
      "source": [
        "def segmentWords(s):\n",
        "    return s.split()\n",
        "\n",
        "def readFile(fileName):\n",
        "    # Function for reading file\n",
        "    # input: filename as string\n",
        "    # output: contents of file as list containing single words\n",
        "    contents = []\n",
        "    f = open(fileName)\n",
        "    for line in f:\n",
        "        contents.append(line)\n",
        "    f.close()\n",
        "    result = segmentWords('\\n'.join(contents))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBqfUHIo8vI_"
      },
      "source": [
        "#### Create a Dataframe containing the counts of each word in a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Hw7UQh1V8vI_"
      },
      "outputs": [],
      "source": [
        "d = []\n",
        "\n",
        "for c in os.listdir(\"data_training/train\"):\n",
        "    directory = \"data_training/train/\" + c\n",
        "    for f in os.listdir(directory):\n",
        "        words = readFile(directory + \"/\" + f)\n",
        "        e = {x:words.count(x) for x in words}\n",
        "        e['__FileID__'] = f\n",
        "        e['__CLASS__'] = 1 if c[:3] == 'pos' else 0\n",
        "        d.append(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dma8ah_68vJA"
      },
      "source": [
        "**Create a dataframe from d - make sure to fill all the nan values with zeros.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2u0Gz6798vJB"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(d).fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Psancu_m8vJC",
        "outputId": "a7d081eb-9911-45e7-f36a-981ab2124871"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1400, 42776)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style>\n",
              "    .dataframe thead tr:only-child th {\n",
              "        text-align: right;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>\u0005</th>\n",
              "      <th>\u0013earth</th>\n",
              "      <th>\u0013goodies</th>\n",
              "      <th>\u0013if</th>\n",
              "      <th>\u0013ripley</th>\n",
              "      <th>\u0013suspend</th>\n",
              "      <th>\u0013they</th>\n",
              "      <th>\u0013white\u0014</th>\n",
              "      <th>\u0014</th>\n",
              "      <th>\u0016</th>\n",
              "      <th>...</th>\n",
              "      <th>zukovsky</th>\n",
              "      <th>zundel</th>\n",
              "      <th>zurg's</th>\n",
              "      <th>zweibel</th>\n",
              "      <th>zwick</th>\n",
              "      <th>zwick's</th>\n",
              "      <th>zwigoff's</th>\n",
              "      <th>zycie</th>\n",
              "      <th>zycie'</th>\n",
              "      <th>|</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 42776 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     \u0005  \u0013earth  \u0013goodies  \u0013if  \u0013ripley  \u0013suspend  \u0013they  \u0013white\u0014    \u0014    \u0016  \\\n",
              "0  0.0     0.0       0.0  0.0      0.0       0.0    0.0      0.0  0.0  0.0   \n",
              "1  0.0     0.0       0.0  0.0      0.0       0.0    0.0      0.0  0.0  0.0   \n",
              "2  0.0     0.0       0.0  0.0      0.0       0.0    0.0      0.0  0.0  0.0   \n",
              "3  0.0     0.0       0.0  0.0      0.0       0.0    0.0      0.0  0.0  0.0   \n",
              "4  0.0     0.0       0.0  0.0      0.0       0.0    0.0      0.0  0.0  0.0   \n",
              "\n",
              "  ...   zukovsky  zundel  zurg's  zweibel  zwick  zwick's  zwigoff's  zycie  \\\n",
              "0 ...        0.0     0.0     0.0      0.0    0.0      0.0        0.0    0.0   \n",
              "1 ...        0.0     0.0     0.0      0.0    0.0      0.0        0.0    0.0   \n",
              "2 ...        0.0     0.0     0.0      0.0    0.0      0.0        0.0    0.0   \n",
              "3 ...        0.0     0.0     0.0      0.0    0.0      0.0        0.0    0.0   \n",
              "4 ...        0.0     0.0     0.0      0.0    0.0      0.0        0.0    0.0   \n",
              "\n",
              "   zycie'    |  \n",
              "0     0.0  0.0  \n",
              "1     0.0  0.0  \n",
              "2     0.0  0.0  \n",
              "3     0.0  0.0  \n",
              "4     0.0  0.0  \n",
              "\n",
              "[5 rows x 42776 columns]"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6ap4t8p8vJD",
        "outputId": "7b426116-1cc3-49ca-8ca8-a1afb88803aa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style>\n",
              "    .dataframe thead tr:only-child th {\n",
              "        text-align: right;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>\u0005</th>\n",
              "      <th>\u0013earth</th>\n",
              "      <th>\u0013goodies</th>\n",
              "      <th>\u0013if</th>\n",
              "      <th>\u0013ripley</th>\n",
              "      <th>\u0013suspend</th>\n",
              "      <th>\u0013they</th>\n",
              "      <th>\u0013white\u0014</th>\n",
              "      <th>\u0014</th>\n",
              "      <th>\u0016</th>\n",
              "      <th>...</th>\n",
              "      <th>zukovsky</th>\n",
              "      <th>zundel</th>\n",
              "      <th>zurg's</th>\n",
              "      <th>zweibel</th>\n",
              "      <th>zwick</th>\n",
              "      <th>zwick's</th>\n",
              "      <th>zwigoff's</th>\n",
              "      <th>zycie</th>\n",
              "      <th>zycie'</th>\n",
              "      <th>|</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.003571</td>\n",
              "      <td>0.008571</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.001429</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.006429</td>\n",
              "      <td>0.002857</td>\n",
              "      <td>0.001429</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.001429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.026726</td>\n",
              "      <td>0.026726</td>\n",
              "      <td>0.026726</td>\n",
              "      <td>0.026726</td>\n",
              "      <td>0.026726</td>\n",
              "      <td>0.026726</td>\n",
              "      <td>0.026726</td>\n",
              "      <td>0.026726</td>\n",
              "      <td>0.080127</td>\n",
              "      <td>0.272517</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026726</td>\n",
              "      <td>0.053452</td>\n",
              "      <td>0.026726</td>\n",
              "      <td>0.026726</td>\n",
              "      <td>0.128058</td>\n",
              "      <td>0.065426</td>\n",
              "      <td>0.037783</td>\n",
              "      <td>0.026726</td>\n",
              "      <td>0.026726</td>\n",
              "      <td>0.037783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 42775 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 \u0005       \u0013earth     \u0013goodies          \u0013if      \u0013ripley  \\\n",
              "count  1400.000000  1400.000000  1400.000000  1400.000000  1400.000000   \n",
              "mean      0.000714     0.000714     0.000714     0.000714     0.000714   \n",
              "std       0.026726     0.026726     0.026726     0.026726     0.026726   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "          \u0013suspend        \u0013they      \u0013white\u0014            \u0014            \u0016  \\\n",
              "count  1400.000000  1400.000000  1400.000000  1400.000000  1400.000000   \n",
              "mean      0.000714     0.000714     0.000714     0.003571     0.008571   \n",
              "std       0.026726     0.026726     0.026726     0.080127     0.272517   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     2.000000    10.000000   \n",
              "\n",
              "          ...          zukovsky       zundel       zurg's      zweibel  \\\n",
              "count     ...       1400.000000  1400.000000  1400.000000  1400.000000   \n",
              "mean      ...          0.000714     0.001429     0.000714     0.000714   \n",
              "std       ...          0.026726     0.053452     0.026726     0.026726   \n",
              "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
              "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
              "50%       ...          0.000000     0.000000     0.000000     0.000000   \n",
              "75%       ...          0.000000     0.000000     0.000000     0.000000   \n",
              "max       ...          1.000000     2.000000     1.000000     1.000000   \n",
              "\n",
              "             zwick      zwick's    zwigoff's        zycie       zycie'  \\\n",
              "count  1400.000000  1400.000000  1400.000000  1400.000000  1400.000000   \n",
              "mean      0.006429     0.002857     0.001429     0.000714     0.000714   \n",
              "std       0.128058     0.065426     0.037783     0.026726     0.026726   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "max       3.000000     2.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "                 |  \n",
              "count  1400.000000  \n",
              "mean      0.001429  \n",
              "std       0.037783  \n",
              "min       0.000000  \n",
              "25%       0.000000  \n",
              "50%       0.000000  \n",
              "75%       0.000000  \n",
              "max       1.000000  \n",
              "\n",
              "[8 rows x 42775 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtSs8xUl8vJE",
        "outputId": "9f7e29ab-41b2-407d-b53d-980c9baa74f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    cv676_22202.txt\n",
            "1     cv155_7845.txt\n",
            "2    cv465_23401.txt\n",
            "3    cv398_17047.txt\n",
            "4    cv206_15893.txt\n",
            "Name: __FileID__, dtype: object\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1395    1\n",
              "1396    1\n",
              "1397    1\n",
              "1398    1\n",
              "1399    1\n",
              "Name: __CLASS__, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(df.__FileID__.head())\n",
        "df.__CLASS__.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by7uvjbM8vJF"
      },
      "source": [
        "# Training/Validation Split\n",
        "Because we don’t have access to the labels of the test set, we randomly shuffle the dataset and split the data into a training set and validation set, so we can test our trained model on the validation set. In general, even if you have access to the labels of the test set, it is a good idea to use a validation set to prevent overfitting to the test set. (Hint: Use train_test_split from sklearn.model_selection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3FO9djn8vJF"
      },
      "source": [
        "#### Split data into training and validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "QKN9Mp0s8vJF"
      },
      "source": [
        "* Sample 80% of your dataframe to be the training data\n",
        "\n",
        "* Let the remaining 20% be the validation data (you can filter out the indicies of the original dataframe that weren't selected for the training data)\n",
        "* Split the dataframe for both training and validation data into x and y dataframes - where y contains the labels and x contains the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TZ05tFqf8vJG"
      },
      "outputs": [],
      "source": [
        "features = df.drop(['__FileID__', '__CLASS__'], axis=1)\n",
        "labels = df.__CLASS__\n",
        "X_train, X_val, Y_train, Y_val = sklearn.model_selection.train_test_split(features, labels, test_size=0.2,\n",
        "                                                                         random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0IQSwR78vJG",
        "outputId": "da169381-6e54-47ae-928a-be6537f0ca87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1120, 42774) (280, 42774) (1120,) (280,)\n"
          ]
        }
      ],
      "source": [
        "# this step was done above before splitting data into training and validation set\n",
        "print(X_train.shape, X_val.shape, Y_train.shape, Y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA4zKy9g8vJH"
      },
      "source": [
        "# Logistic Regression\n",
        "Now we train a basic logistic regression model to classify the sentiment of the reviews. Make sure you do not use the filename as a feature if you previously included it in the Dataframe. Compare the accuracy of this basic model on the training set and the validation set. Are you overfitting? Try changing the parameters of the logistic regression, such as adding a regularization term, to reduce the overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p-YPrUI8vJH",
        "outputId": "ad9058db-45cc-41c7-ad75-02f50c1d18b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train acc: 1.0 \n",
            "Validation acc: 0.839285714286\n"
          ]
        }
      ],
      "source": [
        "logreg = sklearn.linear_model.LogisticRegression()\n",
        "logreg.fit(X_train, Y_train)\n",
        "print(\"Train acc:\", logreg.score(X_train, Y_train), \"\\nValidation acc:\",\n",
        "      logreg.score(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9nx-ert8vJI",
        "outputId": "66d382ad-16a0-4b71-b3d4-9231553cb1c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'C': 1000, 'penalty': 'l1'}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Cs = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "# gamma parameter which inversely controls the standard deviation of our kernel's distribution\n",
        "penalty = ['l1', 'l2']\n",
        "# initialize the dictionary of parameters\n",
        "param_grid = {'C': Cs, 'penalty' : penalty}\n",
        "# initialize the search using input as nfold cross validation\n",
        "lr = sklearn.linear_model.LogisticRegression()\n",
        "search = grid_search.GridSearchCV(lr, param_grid)\n",
        "# fit the search object to our input training data\n",
        "search.fit(X_train, Y_train)\n",
        "# output the best parameters\n",
        "search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXToIoHs8vJJ",
        "outputId": "68c49317-0707-4acd-fac0-05e6f788438a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training acc: 1.0 \n",
            "Validation acc: 0.885714285714\n"
          ]
        }
      ],
      "source": [
        "logreg2 = sklearn.linear_model.LogisticRegression(penalty='l1', C=1000)\n",
        "logreg2.fit(X_train, Y_train)\n",
        "print(\"Training acc:\", logreg2.score(X_train, Y_train), \"\\nValidation acc:\",\n",
        "      logreg2.score(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YwKqvZ88vJJ"
      },
      "source": [
        "Our regularized log reg model still overfits the training data, but it also increased the testing accuracy from 84% to 88.5%, which is most important by a significant amount."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zrl_GFU88vJK"
      },
      "outputs": [],
      "source": [
        "# A recursive feature elimination approach\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# A new logistic regression model with parameters from above and a feature selector\n",
        "lr2 = sklearn.linear_model.LogisticRegression(C=1000, penalty='l1')\n",
        "selector = RFE(lr2, step=10000, n_features_to_select=41000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sIr9dq3_8vJK"
      },
      "outputs": [],
      "source": [
        "# fit RFE selector to training set\n",
        "selector.fit(X_train, Y_train)\n",
        "lr2 = selector.estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2TPh0HRe8vJK"
      },
      "outputs": [],
      "source": [
        "# figure out which columns to drop\n",
        "columns = features.columns\n",
        "feature_mask = selector.support_\n",
        "columns_to_drop = [columns[i] for i in range(columns.size) if not feature_mask[i]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jjVApB-m8vJK"
      },
      "outputs": [],
      "source": [
        "# Create print function to print scores of estimators\n",
        "def print_results(estimator, X, y, leadingString=''):\n",
        "    print(leadingString, estimator.score(X, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx_pKl1x8vJL",
        "outputId": "56540820-f1fe-4a3f-f5d6-ef56220a8f21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training results:  1.0\n",
            "Testing results:  0.892857142857\n"
          ]
        }
      ],
      "source": [
        "# show training and testing accuracies after feature reduction\n",
        "print_results(lr2, X_train.drop(columns_to_drop, axis=1), Y_train, \"Training results: \")\n",
        "print_results(lr2, X_val.drop(columns_to_drop, axis=1), Y_val, \"Testing results: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJRNfPFy8vJL"
      },
      "source": [
        "* We selected features here using a recursive feature elimination model that reduces overfitting by shrinking the hypothesis space of our logistic regression model. This didn't give results that were significantly better at reducing overfitting than the L1  regularization above as the testing accuracy increased from 88.5% to 89.2%, which, when dealing with data at scale, is an increase that may be worth permitting the computation time and cost of running a Backward Stepwise Selection (or feature reduction) method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4De2bApb8vJL"
      },
      "source": [
        "# Single Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3qLWNay8vJM"
      },
      "source": [
        "#### Basic Decision Tree\n",
        "\n",
        "* Initialize model as a decision tree with sklearn.\n",
        "* Fit the data and labels to the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTZMtj_X8vJM",
        "outputId": "9635d894-2f79-423a-ec14-f044fe2939fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training acc: 1.0 \n",
            "Validation acc: 0.635714285714\n"
          ]
        }
      ],
      "source": [
        "dt_clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
        "dt_clf.fit(X_train, Y_train)\n",
        "print(\"Training acc:\", dt_clf.score(X_train, Y_train), \"\\nValidation acc:\", dt_clf.score(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el4vD1iy8vJN",
        "outputId": "23ea53e8-a856-464f-e00e-a18a81ebb5b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score='raise',\n",
              "       estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
              "            max_features=None, max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
              "            splitter='best'),\n",
              "       fit_params=None, iid=True, n_jobs=1,\n",
              "       param_grid={'min_samples_split': [5, 10, 50, 100, 500, 1000], 'min_samples_leaf': [10, 100, 1000, 10000], 'max_depth': [None, 10, 100, 1000, 10000], 'max_leaf_nodes': [None, 10, 100, 1000, 10000]},\n",
              "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
              "       scoring=None, verbose=0)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parameters = {\"max_depth\": [None, 10, 100, 1000, 10000],\n",
        "              \"min_samples_split\": [5, 10, 50, 100, 500, 1000],\n",
        "              \"min_samples_leaf\": [10, 100, 1000, 10000],\n",
        "              \"max_leaf_nodes\": [None, 10, 100, 1000, 10000],\n",
        "              }\n",
        "gridsearch = GridSearchCV(dt_clf, parameters)\n",
        "gridsearch.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnBwJVcv8vJZ",
        "outputId": "6b8df50d-992a-4343-84ef-44513db6b76b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'max_depth': None,\n",
              " 'max_leaf_nodes': 10000,\n",
              " 'min_samples_leaf': 10,\n",
              " 'min_samples_split': 100}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# show the best parameters of the gridsearchCV regularized decision tree\n",
        "gridsearch.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERn1BkvG8vJZ",
        "outputId": "b36c62e4-8e6c-449f-94b3-e9756cadc3b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training acc: 0.757142857143 \n",
            "Validation acc: 0.621428571429\n"
          ]
        }
      ],
      "source": [
        "# use parameters from gridsearchCV above in new decision tree model\n",
        "reg_tree = gridsearch.best_estimator_\n",
        "\n",
        "print(\"Training acc:\", reg_tree.score(X_train, Y_train), \"\\nValidation acc:\",\n",
        "      reg_tree.score(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc3pdY1F8vJa",
        "outputId": "f74e44ef-df65-49aa-fc86-7a99073afe04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training score:  0.858035714286\n",
            "Testing score:  0.692857142857\n"
          ]
        }
      ],
      "source": [
        "# create decision tree model with manually searched parameters (best in class)\n",
        "reg_tree2 = tree.DecisionTreeClassifier(criterion = \"entropy\", max_depth = None, max_leaf_nodes = 125, min_samples_leaf = 2, min_samples_split = 60)\n",
        "reg_tree2.fit(X_train, Y_train)\n",
        "\n",
        "# print model training and test accuracies\n",
        "print_results(reg_tree2, X_train, Y_train, \"Training score: \")\n",
        "print_results(reg_tree2, X_val, Y_val, \"Testing score: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRErxxD08vJb",
        "outputId": "2b48a958-d546-499b-fc7b-f44e83220168"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AdaBoostClassifier(algorithm='SAMME.R',\n",
              "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
              "            max_features=None, max_leaf_nodes=125,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=2, min_samples_split=60,\n",
              "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
              "            splitter='best'),\n",
              "          learning_rate=1.0, n_estimators=100, random_state=None)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# train an AdaBoost classifier using the tuned random forest model above as the base estimator\n",
        "boost_clf = AdaBoostClassifier(base_estimator=reg_tree2, n_estimators=100)\n",
        "boost_clf.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbZ90lTv8vJb",
        "outputId": "fdc7c23b-4b6b-4072-f65b-47aeb917ea70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training acc: 1.0 \n",
            "Validation acc: 0.739285714286\n"
          ]
        }
      ],
      "source": [
        "print(\"Training acc:\", boost_clf.score(X_train, Y_train), \"\\nValidation acc:\",\n",
        "      boost_clf.score(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UijrucFB8vJc"
      },
      "source": [
        "The (AdaBoost) boosted decision tree classifier produces the best testing accuracy in (the decision tree) class at 73%. The model may seem to be overfitting the training data, but it captures the general signal or trend of the test data well enough to predict better than the decision tree base estimator and the gridsearchCV tuned decision tree model. Still, logistic regression, a simpler model, seems to be the better, more accurate option for sentiment analysis with this particular IMDB movie review text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prjQXhO-8vJc"
      },
      "source": [
        "# Random Forest Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VA8ZpeX8vJc"
      },
      "source": [
        "#### Basic Random Forest\n",
        "\n",
        "* Use sklearn's ensemble.RandomForestClassifier() to create your model.\n",
        "* Fit the data and labels with your model.\n",
        "* Score your model with the same data and labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvWcExRQ8vJc"
      },
      "source": [
        "A Random Forest classifier prevents overfitting better than a decision tree by using a series of weaker trees that cannot themselves overfit the training set and takes a majority vote from them to classify. Training an AdaBoost Classifier on the same dataset trains weak learners sequentially, so they focus on the points that are hard to classify, so we made sure to limit each individual tree so it is individually weak."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_iYekgM8vJd",
        "outputId": "43bedaa5-e88d-424e-e1da-247483a8535b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training acc: 1.0 \n",
            "Validation acc: 0.821428571429\n"
          ]
        }
      ],
      "source": [
        "rfc = RandomForestClassifier(criterion = 'entropy', n_estimators=100)\n",
        "rfc.fit(X_train, Y_train)\n",
        "print(\"Training acc:\", rfc.score(X_train, Y_train), \"\\nValidation acc:\",\n",
        "      rfc.score(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWjy1FnJ8vJd"
      },
      "source": [
        "## Changing Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA4ak3F_8vJd"
      },
      "source": [
        "parameters = {\"min_samples_split\": [2, 5, 10],\n",
        "              \"max_depth\": [None, 2, 5, 10],\n",
        "              \"min_samples_leaf\": [1, 5, 10],\n",
        "              \"max_leaf_nodes\": [None, 5, 10, 20],\n",
        "              }\n",
        "gridsearch2 = GridSearchCV(rfc, parameters)\n",
        "gridsearch2.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCHS5MW28vJd",
        "outputId": "b27aadd2-78da-45f6-d629-dee6fa989de0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'max_leaf_nodes': None, 'min_samples_leaf': 10, 'min_samples_split': 100}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gridsearch2.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF-KnV6c8vJe",
        "outputId": "5910c0fc-bf76-408e-b365-8a82cef53a7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training acc: 0.935714285714 \n",
            "Validation acc: 0.817857142857\n"
          ]
        }
      ],
      "source": [
        "reg_forest = gridsearch2.best_estimator_\n",
        "reg_forest.fit(X_train, Y_train)\n",
        "print(\"Training acc:\", reg_forest.score(X_train, Y_train), \"\\nValidation acc:\",\n",
        "      reg_forest.score(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiQ4jomM8vJe"
      },
      "source": [
        "After tuning the parameters of our random forest classifier using sklearn's GridSearchCV method, we decreased the model's overfitting of the training set from a 100% accuracy to a 93% accuracy. However, our test set accuracy on the parameter-tuned model was slightly lower at 81.7%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kofNkrli8vJe"
      },
      "source": [
        "**Add a Boost to Random Forest model with sklearn's AdaBoostClassifier( )**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlLCVNIn8vJf",
        "outputId": "950d81df-b673-405c-8f25-9d971fa52804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training acc: 1.0 \n",
            "Validation acc: 0.882142857143\n"
          ]
        }
      ],
      "source": [
        "boost_reg2 = AdaBoostClassifier(base_estimator=reg_forest)\n",
        "boost_reg2.fit(X_train, Y_train)\n",
        "print(\"Training acc:\", boost_reg2.score(X_train, Y_train), \"\\nValidation acc:\",\n",
        "      boost_reg2.score(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcCw2h5t8vJf"
      },
      "source": [
        "What parameters did you choose to change and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "0d_FGbiO8vJf"
      },
      "source": [
        "We regularized the model parameters by running a standard grid search on the hyperparameters and received the following hyperparameters: min_samples_split=2, max_depth=None, min_samples_leaf=1, and max_leaf_nodes=None. The resulting model brought the training accuracy down to .90 and the validation set accuracy up to .83.\n",
        "\n",
        "Finally, we trained an AdaBoostClassifier model using our regularized random forest as our base_estimator and received a training accuracy of 1.0 and a testing accuracy of .89. Although the model seems to overfit the training data, it also produces our highest testing accuracy yet. We decided to optimize this AdaBoostClassifier model by running a standard grid search on the n_estimators hyperparameter, which informed us that the best resulting model used a value of 50 for n_estimators. When we ran the resulting AdaBoostClassifier model using our regularized random forest as our base estimator and 50 as our number of estimators, we received a 1.0 training accuracy and a testing accuracy of .88, a class best. Again, the AdaBoost classifiers seem to overfit the training data, most likely due to our base estimator (the regularized random forest) being relatively strong estimators rather than the required weak estimators, but our resulting testing accuracy was a class best at .88. Thus, it seems as though a boosted random forest model does not perform better than the simpler, parameter-tuned logistic regression model, which produced a testing accuracy of 89.2%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5Qa1iAQ8vJf"
      },
      "source": [
        "How does a random forest classifier prevent overfitting better than a single decision tree?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "kkIdqi5r8vJf"
      },
      "source": [
        "A Random Forest classifier prevents overfitting better than a decision tree by using a series of weaker trees that cannot themselves overfit the training set and takes a majority vote from them to classify. Training an AdaBoost Classifier on the same dataset trains weak learners sequentially, so they focus on the points that are hard to classify, so we made sure to limit each individual tree so it is individually weak."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}